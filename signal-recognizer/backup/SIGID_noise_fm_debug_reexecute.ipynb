{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SIGID-noise-fm-debug-reexecute.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7pwwrG_z9XD",
        "colab_type": "text"
      },
      "source": [
        "Laburanding en ver de diferenciar una senial de noise de una de FM, para luego extender a varias mas. Version para re ejecutar y dejar el mejor.\n",
        "\n",
        "Data logged using:\n",
        "uhd_ -f -b \n",
        "\n",
        "**Se ajusto todo para debuggear porque no sale de .5 accuracy, ver problema.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdSC_l0QBfZf",
        "colab_type": "code",
        "outputId": "b082a0dc-f9fe-4e4f-e3dd-336e791f9325",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model, Sequential\n",
        "from tensorflow.keras.layers import TimeDistributed, LSTM, Dropout, Conv1D, Dense, Activation, MaxPooling1D, Input\n",
        "from tensorflow.keras.utils import Sequence as Sequence\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "from datetime import datetime\n",
        "import ast\n",
        "\n",
        "sample_rate = 100000\n",
        "signals_count = 2\n",
        "epochs = 20\n",
        "batch_size = 128\n",
        "capture_size = 2048\n",
        "\n",
        "# Debug\n",
        "#train_size = 16\n",
        "#test_size = 4\n",
        "#capture_size = 8\n",
        "# Test\n",
        "#train_size = 50\n",
        "#test_size = 5\n",
        "# Real\n",
        "train_size = 1000\n",
        "test_size = 200\n",
        "\n",
        "dir_base_model = \"/content/gdrive/My Drive/DeepLearning/sigid/test-\"\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            "2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pSRLW-swqu3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Create a dtype with the binary data format and the desired column names\n",
        "dt = np.dtype([('i', 'f4'), ('q', 'f4')])\n",
        "# Load noisy data\n",
        "data = np.fromfile(dir_base_model + \"nothing.iq\", dtype=dt)\n",
        "dfa = pd.DataFrame(data)\n",
        "dfa[\"label\"] = \"BLANK\"\n",
        "npArray = np.array(range(len(dfa)), dtype='f4', copy=True, order='K', subok=False, ndmin=0)\n",
        "dfa[\"time\"] = npArray / sample_rate\n",
        "# Load FM data\n",
        "data = np.fromfile(dir_base_model + \"999.iq\", dtype=dt)\n",
        "dfb = pd.DataFrame(data)\n",
        "dfb[\"label\"] = \"FM\"\n",
        "npArray = np.array(range(len(dfb)), dtype='f4', copy=True, order='K', subok=False, ndmin=0)\n",
        "dfb[\"time\"] = npArray / sample_rate\n",
        "\n",
        "frames = [dfa, dfb]\n",
        "result = pd.concat(frames)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzG66vrpyfhf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = []\n",
        "test = []\n",
        "\n",
        "# Build datasets for train \n",
        "for i in (0, train_size):\n",
        "  ii = i * capture_size\n",
        "  train.append(dfa[ii:ii + capture_size].values)\n",
        "for i in (0, train_size):\n",
        "  ii = i * capture_size\n",
        "  train.append(dfb[ii:ii + capture_size].values)\n",
        "\n",
        "# Build datasets for test \n",
        "for i in (0, test_size):\n",
        "  ii = i * capture_size\n",
        "  test.append(dfa[ii:ii + capture_size].values)\n",
        "for i in (0, test_size):\n",
        "  ii = i * capture_size\n",
        "  test.append(dfb[ii:ii + capture_size].values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRUQTgUr8p9L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Capture size dfa BLANK: ' + str(len(dfa.values)))\n",
        "print('Capture size dfb FM: ' + str(len(dfb.values)))\n",
        "#print(dfa[1:capture_size]['i'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIKmeEHkJmS4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(x=dfa[1:capture_size]['time'], y=dfa[1:capture_size]['i'], mode='lines', name='i'))\n",
        "fig.add_trace(go.Scatter(x=dfa[1:capture_size]['time'], y=dfa[1:capture_size]['q'], mode='lines', name='q'))\n",
        "fig.update_layout(\n",
        "    title=\"NOISE\",\n",
        "    xaxis_title=\"time\",\n",
        "    yaxis_title=\"value\",\n",
        "    font=dict(\n",
        "        family=\"Courier New, monospace\",\n",
        "        size=18,\n",
        "        color=\"#7f7f7f\"\n",
        "    )\n",
        ")\n",
        "fig.show()\n",
        "\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(x=dfb[1:capture_size]['time'], y=dfb[1:capture_size]['i'], mode='lines', name='i'))\n",
        "fig.add_trace(go.Scatter(x=dfb[1:capture_size]['time'], y=dfb[1:capture_size]['q'], mode='lines', name='q'))\n",
        "fig.update_layout(\n",
        "    title=\"FM\",\n",
        "    xaxis_title=\"time\",\n",
        "    yaxis_title=\"value\",\n",
        "    font=dict(\n",
        "        family=\"Courier New, monospace\",\n",
        "        size=18,\n",
        "        color=\"#7f7f7f\"\n",
        "    )\n",
        ")\n",
        "fig.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEcc_eOioZZV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dfa[1:capture_size]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmXGfsTfTyHi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot some sample data to check iq files\n",
        "import plotly.express as px\n",
        "interleave = 2\n",
        "fig = px.line(dfa[1+train_size * interleave + capture_size:capture_size], x=\"time\", y=\"i\", labels={'X':'I'}, width=1000, height=300)\n",
        "fig.show()\n",
        "fig = px.line(dfb[1+train_size * interleave + capture_size:capture_size], x=\"time\", y=\"i\", labels={'X':'I'}, width=1000, height=300)\n",
        "fig.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D40kVty0h_8Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_signal_dataframes(df):\n",
        "  interleave = 2\n",
        "  dx = []\n",
        "  dy = []\n",
        "  tx = []\n",
        "  ty = []\n",
        "\n",
        "  # Generate train dataset \n",
        "  dfax = df[['i', 'q']] \n",
        "  dfay = df[['label']] \n",
        "  dfax = dfax * 10000.0\n",
        "  for i in range(0, train_size):\n",
        "    newx = dfax[interleave * i:interleave * i + capture_size]\n",
        "    dx.append(newx)    \n",
        "    newy = dfay[interleave * i:interleave * i + capture_size]\n",
        "    dy.append(newy)\n",
        "\n",
        "  # Offset to separate train from test datain df\n",
        "  offset = train_size * interleave + capture_size\n",
        "\n",
        "  print(\"Offset \" + str(offset))\n",
        "\n",
        "  # Generate test dataset \n",
        "  for i in range(0, test_size):\n",
        "    newx = dfax[interleave * i + offset:interleave * i + capture_size + offset]\n",
        "    tx.append(newx)\n",
        "    newy = dfay[interleave * i + offset:interleave * i + capture_size + offset]\n",
        "    ty.append(newy)\n",
        "\n",
        "  # Convert lists to dataframes\n",
        "  train_x = pd.DataFrame(dx) \n",
        "  train_y = pd.DataFrame(dy) \n",
        "  test_x = pd.DataFrame(tx) \n",
        "  test_y = pd.DataFrame(ty) \n",
        "  return train_x, train_y, test_x, test_y\n",
        "\n",
        "def add_dataframe_for_signal(df, train_x, train_y, test_x, test_y):\n",
        "  x, y, tx, ty = get_signal_dataframes(df)\n",
        "  train_x.append(x)\n",
        "  train_y.append(y)\n",
        "  test_x.append(tx)\n",
        "  test_y.append(ty)\n",
        "\n",
        "train_x = []\n",
        "train_y = []\n",
        "test_x = []\n",
        "test_y = []\n",
        "add_dataframe_for_signal(dfa, train_x, train_y, test_x, test_y)\n",
        "add_dataframe_for_signal(dfb, train_x, train_y, test_x, test_y)\n",
        "\n",
        "# Build dataframes \n",
        "train_x = pd.concat(train_x)\n",
        "train_y = pd.concat(train_y)\n",
        "test_x = pd.concat(test_x)\n",
        "test_y = pd.concat(test_y)\n",
        "\n",
        "# Shuffle data\n",
        "permutations = np.random.permutation(len(train_y))\n",
        "train_x = train_x.iloc[permutations]\n",
        "train_y = train_y.iloc[permutations]\n",
        "permutations = np.random.permutation(len(test_x))\n",
        "test_x = test_x.iloc[permutations]\n",
        "test_y = test_y.iloc[permutations]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vaj31Pv0DW7T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def recreate_np_array_x(d, size):\n",
        "  new = np.ndarray(shape=(signals_count * size, 2, capture_size), dtype=float)\n",
        "  # train or test size i\n",
        "  for i in range(len(d.values)):\n",
        "    # 2048 j\n",
        "    for j in range(len((d.values)[i])):\n",
        "        new[i][0][j] = ((d.values)[i])[0].values[j][0]\n",
        "        new[i][1][j] = ((d.values)[i])[0].values[j][1]\n",
        "  return new\n",
        "\n",
        "def recreate_np_array_y(d, size):\n",
        "  new = np.ndarray(shape=(signals_count * size), dtype=np.int16)\n",
        "  # 100 i\n",
        "  for i in range(len(d.values)):\n",
        "    result = ((d.values)[i])[0].values[0][0]\n",
        "    if (result == 'FM'):\n",
        "      new[i] = 1\n",
        "    else:\n",
        "      new[i] = 0\n",
        "  return new\n",
        "\n",
        "train_x = recreate_np_array_x(train_x, train_size)\n",
        "train_y = recreate_np_array_y(train_y, train_size)\n",
        "test_x = recreate_np_array_x(test_x, test_size)\n",
        "test_y = recreate_np_array_y(test_y, test_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XAgG58D56vJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Real model to test: .771 accuracay\n",
        "def model_original():\n",
        "  model = Sequential()\n",
        "  model.add(Conv1D(filters=10, kernel_size=16, strides=1, padding='same', activation='relu', input_shape=(2, capture_size)))\n",
        "  model.add(MaxPooling1D(pool_size=10, strides=2, padding='same'))\n",
        "  model.add(Conv1D(filters=10, kernel_size=12, strides=1, padding='same', activation='relu', input_shape=(6, int(capture_size / 2))))\n",
        "  model.add(MaxPooling1D(pool_size=10, strides=2, padding='same'))\n",
        "  model.add(Conv1D(filters=10, kernel_size=6, strides=1, padding='same', activation='relu', input_shape=(6, int(capture_size / 4))))\n",
        "  model.add(MaxPooling1D(pool_size=10, strides=2, padding='same'))\n",
        "  model.add(Conv1D(filters=10, kernel_size=3, strides=1, padding='same', activation='relu', input_shape=(6, int(capture_size / 8))))\n",
        "  model.add(MaxPooling1D(pool_size=10, strides=2, padding='same'))\n",
        "  model.add(Dense(2, activation='softmax'))\n",
        "  model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "# Real model to test: .805 accuracay\n",
        "def model_test():\n",
        "  model = Sequential()\n",
        "  model.add(Conv1D(filters=10, kernel_size=8, strides=3, padding='same', activation='relu', input_shape=(2, capture_size)))\n",
        "  model.add(MaxPooling1D(pool_size=10, strides=2, padding='same'))\n",
        "  model.add(Dense(20, activation='relu'))\n",
        "  model.add(Conv1D(filters=10, kernel_size=12, strides=1, padding='same', activation='relu', input_shape=(6, int(capture_size / 2))))\n",
        "  model.add(MaxPooling1D(pool_size=10, strides=2, padding='same'))\n",
        "  model.add(Dense(20, activation='relu'))\n",
        "  model.add(Dense(2, activation='softmax'))\n",
        "  model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWdtNiyxWg6J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_last_model():\n",
        "  model = tf.keras.models.load_model(dir_base_model + 'best-noise-vs-fm-info-best.model')\n",
        "  model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsWebY1bMj5w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tensorboard test\n",
        "from datetime import datetime\n",
        "logdir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dI89x6H0CsnO",
        "colab_type": "code",
        "outputId": "334cf5c1-7a01-4267-a231-1ac048e86d0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# #TODO: Contar los distintos labels de los df\n",
        "#model = model_original()\n",
        "model = load_last_model()\n",
        "model.summary()\n",
        "\n",
        "# Train\n",
        "model.fit(x = train_x, y = train_y, batch_size=batch_size, verbose=1, validation_data=(test_x, test_y), epochs=60, callbacks=[tensorboard_callback])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_20 (Conv1D)           (None, 2, 10)             327690    \n",
            "_________________________________________________________________\n",
            "max_pooling1d_19 (MaxPooling (None, 1, 10)             0         \n",
            "_________________________________________________________________\n",
            "conv1d_21 (Conv1D)           (None, 1, 10)             1210      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_20 (MaxPooling (None, 1, 10)             0         \n",
            "_________________________________________________________________\n",
            "conv1d_22 (Conv1D)           (None, 1, 10)             610       \n",
            "_________________________________________________________________\n",
            "max_pooling1d_21 (MaxPooling (None, 1, 10)             0         \n",
            "_________________________________________________________________\n",
            "conv1d_23 (Conv1D)           (None, 1, 10)             310       \n",
            "_________________________________________________________________\n",
            "max_pooling1d_22 (MaxPooling (None, 1, 10)             0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 1, 2)              22        \n",
            "=================================================================\n",
            "Total params: 329,842\n",
            "Trainable params: 329,842\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 2000 samples, validate on 400 samples\n",
            "Epoch 1/60\n",
            "2000/2000 [==============================] - 2s 752us/sample - loss: 0.6932 - accuracy: 0.4860 - val_loss: 5.8211 - val_accuracy: 0.4875\n",
            "Epoch 2/60\n",
            "2000/2000 [==============================] - 1s 400us/sample - loss: 0.6932 - accuracy: 0.4940 - val_loss: 5.8211 - val_accuracy: 0.5000\n",
            "Epoch 3/60\n",
            "2000/2000 [==============================] - 1s 400us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 5.8211 - val_accuracy: 0.5000\n",
            "Epoch 4/60\n",
            "2000/2000 [==============================] - 1s 401us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 5.8211 - val_accuracy: 0.5000\n",
            "Epoch 5/60\n",
            "2000/2000 [==============================] - 1s 403us/sample - loss: 0.6932 - accuracy: 0.4840 - val_loss: 5.8211 - val_accuracy: 0.4875\n",
            "Epoch 6/60\n",
            "2000/2000 [==============================] - 1s 403us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 5.8211 - val_accuracy: 0.4875\n",
            "Epoch 7/60\n",
            "2000/2000 [==============================] - 1s 430us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 5.8211 - val_accuracy: 0.4875\n",
            "Epoch 8/60\n",
            "2000/2000 [==============================] - 1s 422us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 5.8211 - val_accuracy: 0.4875\n",
            "Epoch 9/60\n",
            "2000/2000 [==============================] - 1s 417us/sample - loss: 0.6932 - accuracy: 0.4880 - val_loss: 5.8211 - val_accuracy: 0.5000\n",
            "Epoch 10/60\n",
            "2000/2000 [==============================] - 1s 431us/sample - loss: 0.6932 - accuracy: 0.4830 - val_loss: 5.8211 - val_accuracy: 0.5000\n",
            "Epoch 11/60\n",
            "2000/2000 [==============================] - 1s 424us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 5.8211 - val_accuracy: 0.5000\n",
            "Epoch 12/60\n",
            "2000/2000 [==============================] - 1s 424us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 5.8211 - val_accuracy: 0.5000\n",
            "Epoch 13/60\n",
            "2000/2000 [==============================] - 1s 418us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 5.8211 - val_accuracy: 0.5000\n",
            "Epoch 14/60\n",
            "2000/2000 [==============================] - 1s 421us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 5.8211 - val_accuracy: 0.5000\n",
            "Epoch 15/60\n",
            "2000/2000 [==============================] - 1s 431us/sample - loss: 0.6932 - accuracy: 0.4850 - val_loss: 5.8211 - val_accuracy: 0.4875\n",
            "Epoch 16/60\n",
            "2000/2000 [==============================] - 1s 431us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 5.8211 - val_accuracy: 0.4875\n",
            "Epoch 17/60\n",
            "2000/2000 [==============================] - 1s 426us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 5.8211 - val_accuracy: 0.4875\n",
            "Epoch 18/60\n",
            "2000/2000 [==============================] - 1s 428us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 5.8211 - val_accuracy: 0.4875\n",
            "Epoch 19/60\n",
            "2000/2000 [==============================] - 1s 419us/sample - loss: 0.6932 - accuracy: 0.4890 - val_loss: 5.8211 - val_accuracy: 0.5000\n",
            "Epoch 20/60\n",
            "2000/2000 [==============================] - 1s 428us/sample - loss: 0.6932 - accuracy: 0.4970 - val_loss: 5.8211 - val_accuracy: 0.4875\n",
            "Epoch 21/60\n",
            "2000/2000 [==============================] - 1s 423us/sample - loss: 0.6932 - accuracy: 0.4820 - val_loss: 5.8211 - val_accuracy: 0.4875\n",
            "Epoch 22/60\n",
            "2000/2000 [==============================] - 1s 445us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 5.8211 - val_accuracy: 0.4875\n",
            "Epoch 23/60\n",
            "2000/2000 [==============================] - 1s 426us/sample - loss: 0.6932 - accuracy: 0.4820 - val_loss: 5.8211 - val_accuracy: 0.4875\n",
            "Epoch 24/60\n",
            "2000/2000 [==============================] - 1s 423us/sample - loss: 0.6932 - accuracy: 0.4830 - val_loss: 5.8211 - val_accuracy: 0.4875\n",
            "Epoch 25/60\n",
            "2000/2000 [==============================] - 1s 423us/sample - loss: 0.6932 - accuracy: 0.4920 - val_loss: 5.8211 - val_accuracy: 0.5000\n",
            "Epoch 26/60\n",
            "2000/2000 [==============================] - 1s 426us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 5.8211 - val_accuracy: 0.5000\n",
            "Epoch 27/60\n",
            "2000/2000 [==============================] - 1s 412us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 5.8211 - val_accuracy: 0.5000\n",
            "Epoch 28/60\n",
            "2000/2000 [==============================] - 1s 414us/sample - loss: 0.6932 - accuracy: 0.4790 - val_loss: 5.8211 - val_accuracy: 0.4875\n",
            "Epoch 29/60\n",
            "2000/2000 [==============================] - 1s 422us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 5.8211 - val_accuracy: 0.4875\n",
            "Epoch 30/60\n",
            "2000/2000 [==============================] - 1s 424us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 5.8211 - val_accuracy: 0.4875\n",
            "Epoch 31/60\n",
            "2000/2000 [==============================] - 1s 418us/sample - loss: 0.6932 - accuracy: 0.4850 - val_loss: 5.8211 - val_accuracy: 0.5000\n",
            "Epoch 32/60\n",
            "2000/2000 [==============================] - 1s 418us/sample - loss: 0.6932 - accuracy: 0.4780 - val_loss: 5.8211 - val_accuracy: 0.4875\n",
            "Epoch 33/60\n",
            "2000/2000 [==============================] - 1s 417us/sample - loss: 0.6932 - accuracy: 0.4890 - val_loss: 5.8211 - val_accuracy: 0.5000\n",
            "Epoch 34/60\n",
            "2000/2000 [==============================] - 1s 442us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 5.8211 - val_accuracy: 0.5000\n",
            "Epoch 35/60\n",
            "2000/2000 [==============================] - 1s 418us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 5.8211 - val_accuracy: 0.5000\n",
            "Epoch 36/60\n",
            "2000/2000 [==============================] - 1s 411us/sample - loss: 0.6932 - accuracy: 0.4840 - val_loss: 5.8211 - val_accuracy: 0.4875\n",
            "Epoch 37/60\n",
            "2000/2000 [==============================] - 1s 415us/sample - loss: 0.6932 - accuracy: 0.5010 - val_loss: 5.8211 - val_accuracy: 0.5000\n",
            "Epoch 38/60\n",
            "2000/2000 [==============================] - 1s 422us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 5.8211 - val_accuracy: 0.5000\n",
            "Epoch 39/60\n",
            "2000/2000 [==============================] - 1s 428us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 5.8211 - val_accuracy: 0.5000\n",
            "Epoch 40/60\n",
            "2000/2000 [==============================] - 1s 424us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 5.8211 - val_accuracy: 0.5000\n",
            "Epoch 41/60\n",
            "2000/2000 [==============================] - 1s 431us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 5.8211 - val_accuracy: 0.5000\n",
            "Epoch 42/60\n",
            "2000/2000 [==============================] - 1s 427us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 5.8211 - val_accuracy: 0.5000\n",
            "Epoch 43/60\n",
            "2000/2000 [==============================] - 1s 431us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 5.8211 - val_accuracy: 0.5000\n",
            "Epoch 44/60\n",
            "2000/2000 [==============================] - 1s 431us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 5.8211 - val_accuracy: 0.5000\n",
            "Epoch 45/60\n",
            "2000/2000 [==============================] - 1s 433us/sample - loss: 0.6933 - accuracy: 0.4750 - val_loss: 5.8211 - val_accuracy: 0.4875\n",
            "Epoch 46/60\n",
            "2000/2000 [==============================] - 1s 425us/sample - loss: 0.6932 - accuracy: 0.4810 - val_loss: 5.8211 - val_accuracy: 0.5000\n",
            "Epoch 47/60\n",
            "2000/2000 [==============================] - 1s 438us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 5.8211 - val_accuracy: 0.5000\n",
            "Epoch 48/60\n",
            "2000/2000 [==============================] - 1s 419us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 5.8211 - val_accuracy: 0.5000\n",
            "Epoch 49/60\n",
            "2000/2000 [==============================] - 1s 418us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 5.8211 - val_accuracy: 0.4875\n",
            "Epoch 50/60\n",
            "2000/2000 [==============================] - 1s 425us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 5.8211 - val_accuracy: 0.4875\n",
            "Epoch 51/60\n",
            "2000/2000 [==============================] - 1s 417us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 5.8211 - val_accuracy: 0.4875\n",
            "Epoch 52/60\n",
            "2000/2000 [==============================] - 1s 408us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 5.8211 - val_accuracy: 0.4875\n",
            "Epoch 53/60\n",
            "2000/2000 [==============================] - 1s 419us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 5.8211 - val_accuracy: 0.4875\n",
            "Epoch 54/60\n",
            "2000/2000 [==============================] - 1s 424us/sample - loss: 0.6932 - accuracy: 0.4740 - val_loss: 5.8211 - val_accuracy: 0.5000\n",
            "Epoch 55/60\n",
            "2000/2000 [==============================] - 1s 440us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 5.8211 - val_accuracy: 0.5000\n",
            "Epoch 56/60\n",
            "2000/2000 [==============================] - 1s 428us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 5.8211 - val_accuracy: 0.5000\n",
            "Epoch 57/60\n",
            "2000/2000 [==============================] - 1s 419us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 5.8211 - val_accuracy: 0.5000\n",
            "Epoch 58/60\n",
            "2000/2000 [==============================] - 1s 425us/sample - loss: 0.6932 - accuracy: 0.4940 - val_loss: 5.8211 - val_accuracy: 0.4875\n",
            "Epoch 59/60\n",
            "2000/2000 [==============================] - 1s 422us/sample - loss: 0.6932 - accuracy: 0.4790 - val_loss: 5.8211 - val_accuracy: 0.4875\n",
            "Epoch 60/60\n",
            "2000/2000 [==============================] - 1s 428us/sample - loss: 0.6932 - accuracy: 0.4800 - val_loss: 5.8211 - val_accuracy: 0.5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f342ae96668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ai9zKrqhhf80",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_saved_info():\n",
        "  try:\n",
        "      file = open(dir_base_model + 'best-noise-vs-fm-info-best.txt', 'rt')\n",
        "      value = file.read()\n",
        "      print(value)\n",
        "      file.close()\n",
        "      old_evaluation_result = ast.literal_eval(value)\n",
        "      return old_evaluation_result\n",
        "  except FileNotFoundError:\n",
        "      return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxiyvBKCgt9n",
        "colab_type": "code",
        "outputId": "3aab1657-7b82-4eb6-adcd-1002a274f89a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "source": [
        "# Evaluate model\n",
        "evaluation_result = model.evaluate(test_x, test_y, verbose=1)\n",
        "print(evaluation_result)\n",
        "print(\"Current loss is: \" + str(evaluation_result))\n",
        "\n",
        "old_evaluation_result = read_saved_info()\n",
        "exists = False\n",
        "print(\"Old loss is: \" + str(old_evaluation_result))\n",
        "if (old_evaluation_result != None):\n",
        "  exists = True\n",
        "\n",
        "if (exists == False or (evaluation_result[1] > old_evaluation_result[1])):\n",
        "  print(\"Writting new object becauce of better validation accuracy...\")\n",
        "  file = open(dir_base_model + 'best-noise-vs-fm-info-best.txt', 'wt')\n",
        "  file.write(str(evaluation_result))\n",
        "  file.close()\n",
        "  model.save(dir_base_model + 'best-noise-vs-fm-info-best.model')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "400/400 [==============================] - 0s 164us/sample - loss: 5.8211 - accuracy: 0.5000\n",
            "[5.8211079645156865, 0.5]\n",
            "Current loss is: [5.8211079645156865, 0.5]\n",
            "[0.44500040531158447, 0.79]\n",
            "Old loss is: [0.44500040531158447, 0.79]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUKBKR_0jmK8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare one line of data to predict\n",
        "iv = test_x[0][0]\n",
        "qv = test_x[0][1]\n",
        "element = 0\n",
        "\n",
        "td = np.ndarray(shape=(2, 2, capture_size), dtype=float)\n",
        "for i in range(len(iv)):\n",
        "  td[0][0][i] = test_x[element][0][i]\n",
        "  td[0][1][i] = test_x[element][1][i]\n",
        "for i in range(len(iv)):\n",
        "  td[1][0][i] = test_x[element+1][0][i]\n",
        "  td[1][1][i] = test_x[element+1][1][i]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uY6GbFOBp_nV",
        "colab_type": "code",
        "outputId": "21d08017-c3c0-4cef-8cc4-60fbf8ae51fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "predictions = model.predict(td)\n",
        "print(predictions)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[1. 0.]]\n",
            "\n",
            " [[1. 0.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IevSiFy9wUNO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Draw confusion matrix\n",
        "# https://plot.ly/~francoisp/50/confusion-matrix/#code\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}